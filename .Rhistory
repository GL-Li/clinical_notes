y = unit(1.4, "lines"),
hjust = 0,
vjust = 1,
gp = gpar(col = "red")),
textGrob(label = " and ",
name = "title2",
x = grobWidth("title1") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
hjust = 0,
vjust = 1),
textGrob(label = "Percentage",
name = "title3",
x = grobWidth("title1") + grobWidth("title2") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
gp = gpar(col = "purple"),
hjust = 0,
vjust = 1),
textGrob(label = " of True Sample Types Being Predicted as Other Types",
x = grobWidth("title1") + grobWidth("title2") + grobWidth("title3") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
hjust = 0,
vjust = 1)
)
gg <- arrangeGrob(main_plot, top=grobs, padding = unit(2.6, "line"))
grid.arrange(gg)
# prepare data starting from medical note text
dat <- fread("data/mtsamples_multi_class.csv") %>%
.[, note := str_replace_all(note, "\\.", "\\. ")] %>%
.[, y := as.integer(factor(specialty)) - 1] %>%
.[specialty != "Radiology"]
notes <- dat[, note]
# initialize tokenizer specifing maximum words
max_words <- 3000
tk <- text_tokenizer(num_words = max_words)
# update tk in place with a vector or list of documents
fit_text_tokenizer(tk, notes)
# convert the documents into a list of sequence
X <- texts_to_sequences(tk, notes)
# examine sequence length, the longest is 2471, mean 430
len <- sapply(X, function(x) length(x))
summary(len)
# pad the sequence to get a matrix
seq_length <- 500
X <- pad_sequences(X, seq_length)
y_class <- dat[, y]
n_class <- length(unique(y_class))
y <- to_categorical(y_class, n_class)
# split X and y into train and test
set.seed(1234)
# prepare data starting from medical note text
dat <- fread("data/mtsamples_multi_class.csv") %>%
.[, note := str_replace_all(note, "\\.", "\\. ")] %>%
.[specialty != "Radiology"] %>%
.[, y := as.integer(factor(specialty)) - 1]
notes <- dat[, note]
# initialize tokenizer specifing maximum words
max_words <- 3000
tk <- text_tokenizer(num_words = max_words)
# update tk in place with a vector or list of documents
fit_text_tokenizer(tk, notes)
# convert the documents into a list of sequence
X <- texts_to_sequences(tk, notes)
# examine sequence length, the longest is 2471, mean 430
len <- sapply(X, function(x) length(x))
summary(len)
# pad the sequence to get a matrix
seq_length <- 500
X <- pad_sequences(X, seq_length)
y_class <- dat[, y]
n_class <- length(unique(y_class))
y <- to_categorical(y_class, n_class)
# split X and y into train and test
set.seed(1234)
in_train <- sample(1:nrow(X), round(0.7 * nrow(X)))
in_test <- setdiff(1:nrow(X), in_train) %>%
sample()  # to shuffle the row numbers
X_train <- X[in_train,]
y_train <- y[in_train,]
X_test <- X[in_test,]
y_test <- y[in_test,]
y_test_class <- y_class[in_test]
dim_emb <- 64
dropout <- 0.3
model <- keras_model_sequential() %>%
layer_embedding(input_dim = max_words,
output_dim = dim_emb,
input_length = seq_length) %>%
layer_dropout(dropout) %>%
layer_conv_1d(filters = 16,
kernel_size = 3,
#activation = "relu",
padding = "valid",
strides = 1) %>%
layer_dropout(dropout) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(dropout) %>%
# output layer
layer_dense(n_class, activation = "softmax")
summary(model)
# compile, fit, and evaluate model in place
compile(model,
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
fit(model,
x = X_train, y = y_train,
epochs = 30,
batch_size = 32,
validation_split = 0.3,
verbose = 2
)
evaluate(model, X_test, y_test, verbose = 0)
pred <- predict(model, X_test)
pred_class <- predict_classes(model, X_test)
cm <- table(y_test_class, pred_class)  # confusion matrix
n <- nrow(cm)
# percent
cm_pct <- cm / rowSums(cm)
pct_dt <-  as.data.table(matrix(unlist(cm_pct), ncol = 1)) %>%
.[, x := rep(0:(n-1), n)] %>%
.[, y := rep(0:(n-1), each = n)]
# count
cm_dt <- as.data.table(matrix(unlist(cm), ncol = 1)) %>%
.[, x := rep(0:(n-1), n)] %>%
.[, y := rep(0:(n-1), each = n)]
classes <- unique(dat$specialty) %>%
str_replace(" / ", "\n")
main_plot <- ggplot() +
geom_jitter(aes(y_test_class, pred_class), color = "blue", size = 1,
width = 0.1, height = 0.1, alpha = 0.3) +
geom_text(data = cm_dt, aes(x - 0.05, y + 0.3, label = V1), hjust = 1,
color = "red") +
geom_text(data = pct_dt,
aes(x + 0.05, y + 0.3, label = paste0(round(100 * V1, 1), "%")),
color = "purple", hjust = 0) +
scale_x_continuous(breaks = 0:6, labels = classes) +
scale_y_continuous(breaks = 0:6, labels = classes) +
labs(x = "True Sample Type",
y = "Predicted Sample Type")
grobs <- grobTree(
gp = gpar(fontsize = 12, fontface = "bold"),
textGrob(label = "    Number",
name = "title1",
x = unit(0.2, "lines"),
y = unit(1.4, "lines"),
hjust = 0,
vjust = 1,
gp = gpar(col = "red")),
textGrob(label = " and ",
name = "title2",
x = grobWidth("title1") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
hjust = 0,
vjust = 1),
textGrob(label = "Percentage",
name = "title3",
x = grobWidth("title1") + grobWidth("title2") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
gp = gpar(col = "purple"),
hjust = 0,
vjust = 1),
textGrob(label = " of True Sample Types Being Predicted as Other Types",
x = grobWidth("title1") + grobWidth("title2") + grobWidth("title3") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
hjust = 0,
vjust = 1)
)
gg <- arrangeGrob(main_plot, top=grobs, padding = unit(2.6, "line"))
grid.arrange(gg)
main_plot <- ggplot() +
geom_jitter(aes(y_test_class, pred_class), color = "blue", size = 1,
width = 0.1, height = 0.1, alpha = 0.3) +
geom_text(data = cm_dt, aes(x - 0.05, y + 0.3, label = V1), hjust = 1,
color = "red") +
geom_text(data = pct_dt,
aes(x + 0.05, y + 0.3, label = paste0(round(100 * V1, 1), "%")),
color = "purple", hjust = 0) +
scale_x_continuous(breaks = 0:5, labels = classes) +
scale_y_continuous(breaks = 0:5, labels = classes) +
labs(x = "True Sample Type",
y = "Predicted Sample Type")
grobs <- grobTree(
gp = gpar(fontsize = 12, fontface = "bold"),
textGrob(label = "    Number",
name = "title1",
x = unit(0.2, "lines"),
y = unit(1.4, "lines"),
hjust = 0,
vjust = 1,
gp = gpar(col = "red")),
textGrob(label = " and ",
name = "title2",
x = grobWidth("title1") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
hjust = 0,
vjust = 1),
textGrob(label = "Percentage",
name = "title3",
x = grobWidth("title1") + grobWidth("title2") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
gp = gpar(col = "purple"),
hjust = 0,
vjust = 1),
textGrob(label = " of True Sample Types Being Predicted as Other Types",
x = grobWidth("title1") + grobWidth("title2") + grobWidth("title3") + unit(0.2, "lines"),
y = unit(1.4, "lines"),
hjust = 0,
vjust = 1)
)
gg <- arrangeGrob(main_plot, top=grobs, padding = unit(2.6, "line"))
grid.arrange(gg)
shiny::runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
?tabPanel
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
tabItem()
?tabItem
library(data.table)
library(tidyverse)
library(tidytext)
# define function to extract top words using term frequency and tfidf
get_tfidf <- function(df, col){
# df: dataframe containing a column of corpus
# col: string, column name of the corpus selected for tfidf
# count of a word in a document
tokens <- as_tibble(df) %>%
select(id, !!col) %>%
unnest_tokens(word, !!col) %>%
anti_join(stop_words) %>%
count(id, word, sort = TRUE) %>%
# keep words with letters only
filter(str_detect(word, "^[a-z]+$"))
# times of appearance of a word
n_times <- tokens %>%
group_by(word) %>%
summarise(n_times = sum(n))
# words only shows up one time
words_1 <- n_times %>%
filter(n_times == 1) %>%
select(word) %>%
pull()
# remove the one-time words, which are note represenative
tokens <- tokens %>%
filter(!word %in% words_1)
# total words in a document
total_words <- tokens %>%
group_by(id) %>%
summarise(total = sum(n))
df_tfidf <- left_join(tokens, total_words) %>%
bind_tf_idf(word, id, n)
# stats of a word
word_stats <- df_tfidf %>%
group_by(word) %>%
summarise(n_documents = n(),
n_times = sum(n),
avg_tf = round(mean(tf),4),
avg_tfidf = round(mean(tf_idf), 4))
# top 10 words by term frequency in each document
top_tf <- df_tfidf %>%
arrange(desc(tf)) %>%
group_by(id) %>%
slice(1:10) %>%
select(id, word) %>%
group_by(id) %>%
summarise(top_tf = paste(word, collapse = " "))
# top 10 words by tfidf
top_tfidf <- df_tfidf %>%
arrange(desc(tf_idf)) %>%
group_by(id) %>%
slice(1:10) %>%
select(id, word) %>%
group_by(id) %>%
summarise(top_tfidf = paste(word, collapse = " "))
sum_tfidf <- df_tfidf %>%
group_by(id) %>%
summarise(sum_tfidf = round(sum(tf_idf), 3)) %>%
left_join(top_tf) %>%
left_join(top_tfidf) %>%
right_join(df) %>%
arrange(id)
return(list(tfidf = sum_tfidf, word_stats = word_stats))
}
# get tf and tfidf words
notes <- fread("./data/mtsample_gastroenterology_neurology.csv") %>%
# missing space after ".", for example "abscess.PROCEDURE"
.[, note := str_replace_all(note, "\\.", "\\. ")]
tfidf_list <- get_tfidf(notes, "note")
sum_tfidf <- tfidf_list[["tfidf"]] %>%
setDT() %>%
setkey("id")
# save to RData ==========================================
word_stats <- tfidf_list[["word_stats"]]
# get medical entities created by Amazon Comprehend Medical
amazon_me <- fread("amazon_medical_entities.csv") %>%
setkey("id") %>%
.[, amazon_me := tolower(amazon_me)]
# get medical entities created by Amazon Comprehend Medical
amazon_me <- fread("./data/amazon_medical_entities.csv") %>%
setkey("id") %>%
.[, amazon_me := tolower(amazon_me)]
# get medical entites created by medaCy
medacy_me <- fread("./data/medacy_medical_entities.csv") %>%
setkey("id") %>%
.[, medacy_me := tolower(medacy_me)]
# combine all for shiny server to save to RData ==========
note_bows <- sum_tfidf %>%
medacy_me[.] %>%
amazon_me[.]
names(note_bows)
# save to RData ===============================================================
save(note_bows, file = "./shiny-apps/RData/medical_entities_bow.RData")
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
# scrape data of one sample
scrape_one_sample <- function(sample_url){
sample_page <- read_html(sample_url)
# get string of section titles seperated by ","
sections <- sample_page %>%
html_nodes(css = "b") %>%
html_text() %>%
str_extract("[A-Z][A-Z /]+[A-Z]") %>%
#str_remove(":") %>%
.[!is.na(.)] %>%
setdiff("NOTE") %>%
paste(collapse = ", ")
# get all text of the sample. Other contents are extracted from sample_text
sample_text <- sample_page %>%
html_node(xpath = '//*[@id="sampletext"]') %>%
html_text() %>%
# dotall = TRUE to match . to \r and \n
str_remove(regex("^.+(?=Sample Type)", dotall=TRUE))
# extract everything between "Medical Specialty: " and "Sample Name: "
sample_type <- str_extract(sample_text, "(?<=Medical Specialty:).+(?=Sample Name:)") %>%
str_trim() %>% str_squish()
sample_name <- str_extract(sample_text, "(?<=Sample Name:).+(?=\r\n)") %>%
str_trim() %>% str_squish()
sample_text_1 <- str_remove(sample_text, "^.*\r\n")
description <- str_extract(sample_text_1, "(?<=Description:).+(?=\r\n)") %>%
str_trim() %>% str_squish()
sample_text_2 <- str_remove(sample_text_1, "^.*\r\n")
transcription <- str_remove(sample_text_2, "^.+Report\\)") %>%
str_remove("^[\r\n ]*") %>%
str_extract("^.*(?=\r\n)") %>%
str_remove_all("\t") %>%
str_trim() %>% str_squish()
keywords <- str_extract(sample_text, "(?<=Keywords: \r\n).*(?=\r\n)") %>%
str_trim() %>% str_squish()
# rename for easy use later
df <- data.frame(
specialty = sample_type,  # sample type / medical specialty
name = sample_name,
description = description,
note = transcription,  # clinical note
sections = sections,
keywords = keywords,
stringsAsFactors = FALSE
)
}
sample_url <- "https://www.mtsamples.com/site/pages/sample.asp?Type=85-Surgery&Sample=1233-Adenocarcinoma%20&%20Mesothelioma"
aaa <- scrape_one_sample(sample_url)
library(rvest)
library(stringr)
aaa <- scrape_one_sample(sample_url)
View(aaa)
# get url to each sample in one page
get_sample_urls <- function(page_url){
sample_urls <- read_html(page_url) %>%
html_nodes(xpath = '//*[@id="Browse"]') %>%
html_nodes("a") %>%
html_attr("href") %>%
paste0("https://www.mtsamples.com", .) %>%
# replace " " with "%20" for legal url
str_replace_all(" ", "%20")
return(sample_urls)
}
# scrape all samples in one page
scrape_one_page <- function(page_url){
df_page <- data.frame(
specialty = character(0),
name = character(0),
description = character(0),
note = character(0),
sections = character(0),
keywords = character(0),
stringsAsFactors = FALSE
)
for(sample_url in get_sample_urls(page_url)){
df <- scrape_one_sample(sample_url)
df_page <- rbind(df_page, df)
}
return(df_page)
}
page_url <- "https://www.mtsamples.com/site/pages/browse.asp?type=21%2DEndocrinology&page=2"
ccc <- scrape_one_page(page_url)
View(ccc)
type_url <- "https://www.mtsamples.com/site/pages/browse.asp?type=85-Surgery"
ddd <- get_number_pages(type_url)
# get the number of pages of a sample type / medical specialty
get_number_pages <- function(type_url){
text <- read_html(type_url) %>%
html_node(xpath = '//*[@id="wrapper"]') %>%
html_text() %>%
str_remove_all("[\r|\n\t]")
if(str_detect(text, ">\\s+>>")){
num <- str_extract(text, "[0-9]+(?=\\s+>\\s+>>)") %>%
as.integer()
return(num)
} else {
return(1)
}
}
type_url <- "https://www.mtsamples.com/site/pages/browse.asp?type=85-Surgery"
ddd <- get_number_pages(type_url)
# get the url of each page of a Sample Type / Medical Specialty using the first
# page url of a Sample Type Medical specialty
get_page_urls <- function(type_url){
number_pages <- get_number_pages(type_url)
if (number_pages == 1){
page_urls <- type_url
} else {
page_urls <- type_url
for(i in 2:number_pages){
url_i <- paste0(type_url, "&page=", i)
page_urls <- c(page_urls, url_i)
}
}
return(page_urls)
}
type_url <- "https://www.mtsamples.com/site/pages/browse.asp?type=85-Surgery"
eee <- get_page_urls(type_url)
eee
# get the url for the first page of each Sample Type / Medical Specialty from
# https://www.mtsamples.com/
get_type_urls <- function(home_url){
home_text <- read_html(home_url) %>%
html_node(xpath = '//*[@id="MenuTypeLeft"]') %>%
html_nodes("a") %>%
html_attr("href") %>%
paste0("https://www.mtsamples.com", .) %>%
# replace " " with "%20" for legal url
str_replace_all(" ", "%20")
return(home_text)
}
home_url <- "https://www.mtsamples.com/"
fff <- get_type_urls(home_url)
fff
library(progress)
sample_page <- read_html(sample_url)
sample_page
# get string of section titles seperated by ",". css selector works here
sections <- sample_page %>%
html_nodes(css = "b") %>%
html_text() %>%
str_extract("[A-Z][A-Z /]+[A-Z]") %>%
#str_remove(":") %>%
.[!is.na(.)] %>%
setdiff("NOTE") %>%
paste(collapse = ", ")
# get all text of the sample. Other contents are extracted from sample_text
sample_text <- sample_page %>%
html_node(xpath = '//*[@id="sampletext"]') %>%
html_text() %>%
# dotall = TRUE to match . to \r and \n
str_remove(regex("^.+(?=Sample Type)", dotall=TRUE))
sample_text
dump(sample_text)
?dump
dput(sample_text)
?file.exists
file.exists("data")
dir.exists("data")
dir.create("aaa")
# save the scraped data
csv_file <- paste0("./data/mtsample_", str_remove_all(Sys.Date(), "-"), ".csv")
cat(paste0("Scraped data saved successfully to ", csv_file))
source('~/Dropbox/work-with-health-data/clinical_notes/web_scrabing_mtsample_com.R')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
library(data.table)
library(magrittr)
