panel.grid.minor = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank(),
axis.text = element_text(size = 12),
panel.background = element_rect(fill = "transparent"),
plot.background = element_rect(fill = "transparent", color = NA)
)
?icon
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
runApp('shiny-apps')
library(data.table)
library(magrittr)
load("./shiny-apps/RData/saved.RData")
View(note_bows)
top_tf <- note_bows[, word]
top_tf <- note_bows[, top_tf]
top_tf <- note_bows[, top_tf] %>%
list()
top_tf <- note_bows[, top_tf] %>%
split()
?split
top_tf <- note_bows[, .(top_tf)] %>%
split()
top_tf <- note_bows[, .(top_tf)] %>%
list()
top_tf <- note_bows[, .(top_tf)]
library(tm)
install.packages("tm")
library(tm)
dt <- note_bows
corpus = Corpus(VectorSource(dt[, get(col)]))
col <- "top_tf"
corpus = Corpus(VectorSource(dt[, get(col)]))
corpus
corpus[[1]]
corpus = tm_map(corpus, tolower)
corpus[[1]]
corpus = tm_map(corpus, PlainTextDocument)
frequencies = DocumentTermMatrix(corpus)
corpus[[1]]
inspect(corpus[[1]])
corpus[[1]]
corpus = Corpus(VectorSource(dt[, get(col)]))
inspect(corpus[[1]])
corpus = tm_map(corpus, tolower)
inspect(corpus[[1]])
frequencies = DocumentTermMatrix(corpus)
sparse = removeSparseTerms(frequencies, 0.995)
dt <- as.data.table(as.matrix(sparse))
View(dt)
View(dt)
?removeSparseTerms
frequencies = DocumentTermMatrix(corpus)
dt <- as.data.table(as.matrix(sparse))
corpus = Corpus(VectorSource(dt[, get(col)]))
corpus = tm_map(corpus, tolower)
frequencies = DocumentTermMatrix(corpus)
dt <- as.data.table(as.matrix(sparse))
vectorize_text <- function(dt, col, sparsity = 0.995){
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower)
frequencies = DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
as.data.table(as.matrix(frequencies))
}
top_tf <- vectorize_text(note_bows, "top_if")
top_tf <- vectorize_text(note_bows, "top_tf")
identical(df, top_tf)
class(dt)
str(dt)
names(dt) == names(top_tf)
amazon_me <- vectorize_text(note_bows, "amazon_me")
View(amazon_me)
?kmeans
# kmeans clustering
k <- kmeans(top_tf, 2)
View(k)
k
target <- note_bows$sample_type
target
target <- note_bows$sample_type %>%
ifelse(. == "Gastroenterology", 1, 2)
note_bows[, target := ifelse(sample_type == "Gastroenterology", 1, 2)]
target <- note_bows$target
target
# kmeans clustering
k_top_tf <- kmeans(top_tf, 2)
k_top_tf
str(k_top_tf)
k_top_tf$cluster
library(caret)
install.packages("caret")
library(caret)
confusionMatrix(k_top_tf$cluster, target)
confusionMatrix(as.factor(k_top_tf$cluster), as.factor(target))
k_amazon_me <- kmeans(amazon_me, 2)
confusionMatrix(as.factor(k_amazon_mef$cluster), as.factor(target))
confusionMatrix(as.factor(k_amazon_me$cluster), as.factor(target))
top_tfid <- vectorize_text(note_bows, "top_tfidf")
top_tfidf <- vectorize_text(note_bows, "top_tfidf")
k_top_tfidf <- kmeans(top_tfidf, 2)
confusionMatrix(as.factor(k_top_tfidf$cluster), as.factor(target))
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(removeWords, stopwords("english"))
# covert text to datatable of frequency ========================================
vectorize_text <- function(dt, col, sparsity = 0.995){
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(removeWords, stopwords("english"))
frequencies = DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
as.data.table(as.matrix(frequencies))
}
top_tf <- vectorize_text(note_bows, "top_tf")
amazon_me <- vectorize_text(note_bows, "amazon_me")
# kmeans clustering
dt <- top_tf
k <- kmeans(dt, 2)
confusionMatrix(as.factor(k$cluster), target)
confusionMatrix(as.factor(k$cluster), as.factor(target))
k_top_tf <- kmeans(top_tf, 2)
confusionMatrix(as.factor(k_top_tf$cluster), as.factor(target))
# covert text to datatable of frequency ========================================
vectorize_text <- function(dt, col, sparsity = 0.995){
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower)
frequencies = DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
as.data.table(as.matrix(frequencies))
}
k_top_tf <- kmeans(top_tf, 2)
confusionMatrix(as.factor(k_top_tf$cluster), as.factor(target))
k_top_tf <- kmeans(top_tf, 2, iter.max = 100)
confusionMatrix(as.factor(k_top_tf$cluster), as.factor(target))
# kmeans clustering
dt <- top_tf
k <- kmeans(dt, 2, iter.max = 1000)
confusionMatrix(as.factor(k$cluster), as.factor(target))
# kmeans clustering
dt <- top_tf
k <- kmeans(dt, 2, iter.max = 10000)
confusionMatrix(as.factor(k$cluster), as.factor(target))
k <- kmeans(dt, 2, iter.max = 1000)
confusionMatrix(as.factor(k$cluster), as.factor(target))
get_kmeans <- function(dt, iter=1000){
k <- kmeans(dt, 2, iter.max = iter)
confusionMatrix(as.factor(k$cluster), as.factor(target))
}
get_kmeans(top_tf)
rep(0, 3)
1:3 + 2:4
round(1.3)
round(1.5)
?round
round(c(1.2, 3.7, 2.5))
get_kmeans <- function(dt, iter=1000, n_rep=100){
pred <- rep(0, nrow(dt))
for (i in n_rep){
k <- kmeans(dt, 2, iter.max = iter)
pred <- pred + k$cluster / nrow(dt)
}
pred <- round(pred)
confusionMatrix(as.factor(pred), as.factor(target))
}
get_kmeans(top_tf)
iter = 1000
n_rep = 100
pred <- rep(0, nrow(dt))
for (i in n_rep){
k <- kmeans(dt, 2, iter.max = iter)
pred <- pred + k$cluster / nrow(dt)
}
pred <- round(pred)
pred
nrow(dt)
for (i in 1:n_rep){
k <- kmeans(dt, 2, iter.max = iter)
pred <- pred + k$cluster / nrow(dt)
}
pred <- round(pred)
pred
k <- kmeans(dt, 2, iter.max = iter)
k$cluster
k$cluster / nrow(dt)
pred <- pred + k$cluster / nrow(dt)
pred <- rep(0, nrow(dt))
for (i in 1:n_rep){
k <- kmeans(dt, 2, iter.max = iter)
pred <- pred + k$cluster / n_rep
}
pred <- round(pred)
confusionMatrix(as.factor(pred), as.factor(target))
get_kmeans <- function(dt, iter=1000, n_rep=1000){
pred <- rep(0, nrow(dt))
for (i in 1:n_rep){
k <- kmeans(dt, 2, iter.max = iter)
pred <- pred + k$cluster / n_rep
}
pred <- round(pred)
confusionMatrix(as.factor(pred), as.factor(target))
}
get_kmeans(top_tf)
get_kmeans(top_tfidf)
get_kmeans <- function(dt, iter=100, n_rep=100){
pred <- rep(0, nrow(dt))
for (i in 1:n_rep){
k <- kmeans(dt, 2, iter.max = iter)
pred <- pred + k$cluster / n_rep
}
pred <- round(pred)
confusionMatrix(as.factor(pred), as.factor(target))
}
get_kmeans(top_tf)
get_kmeans(top_tfidf)
get_kmeans(amazon_me)
View(top_tf)
View(top_tfidf)
View(amazon_me)
all_word <- vector(note_bows, "medical_note")
shiny::runApp('shiny-apps')
runApp('shiny-apps')
source('~/Dropbox/work-with-health-data/medical-notes/shiny-apps/preprocess_data_for_RData/preprocess_data_and_save_RData.R')
?tm_map
?removePunctuation
aaa = Corpus(VectorSource(c("das dssaf das-aa ad.bg", "sdae asd--adsf .adfs.adf")))
aaa
inspect(aaa[[1]])
bbb = tm_map(aaa, removePunctuation)
inspect(bbb[[1]])
str_replace("a.b", "\\.", " ")
# Using the full text tf and tfidf =============================================
dt <- note_bows[, medical_note := str_replace_all(medical_note, "\\.", " ")]
stopwords(("english"))
?DocumentTermMatrix
col = "medical_note"
# make sure dt$col are clean and readable data
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(stripWhitespace) %>%
# remove stopwords before removing punctuationo so that stopwords like
# it's and i'll can be removed
tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument)
inspect(corpus[[1]])
tf <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity) %>%
as.data.table(as.matrix())
sparsity = 0.995
tf <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity) %>%
as.data.table(as.matrix())
corpus
tf <- TermDocumentMatrix(corpus) %>%
removeSparseTerms(sparsity) %>%
as.data.table(as.matrix())
?DocumentTermMatrix
tf <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
tf
aaa = as.matrix(tf)
bbb = as.data.table(aaa)
View(bbb)
tf <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity) %>%
as.matrix() %>%
as.data.table()
tfidf <- DocumentTermMatrix(
corpus,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
) %>%
removeSparseTerms(sparsity) %>%
as.matrix() %>%
as.data.table()
names(tf)
get_tfidf <- function(dt, col, sparsity = 0.995){
# make sure dt$col are clean and readable data
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(stripWhitespace) %>%
# remove stopwords before removing punctuationo so that stopwords like
# it's and i'll can be removed
tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument)
tf <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity) %>%
as.matrix() %>%
as.data.table()
tfidf <- DocumentTermMatrix(
corpus,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
) %>%
removeSparseTerms(sparsity) %>%
as.matrix() %>%
as.data.table()
return(list(tf = tf, tfidf = tfidf))
}
aaa <- get_tfidf(note_bows, "medical_note")
tf <- aaa[["tf"]]
tfidf <- aaa[["tfidf"]]
# kmeans
get_kmeans(tf)
get_kmeans(tfidf)
install.packages("progress")
library(progress)
pb <- progress_bar$new(total = n_rep)
get_kmeans <- function(dt, iter=100, n_rep=100){
pred <- rep(0, nrow(dt))
pb <- progress_bar$new(total = n_rep)
for (i in 1:n_rep){
pb$tick()
k <- kmeans(dt, 2, iter.max = iter)
pred <- pred + k$cluster / n_rep
}
pred <- round(pred)
confusionMatrix(as.factor(pred), as.factor(target))
}
get_kmeans(tfidf)
get_tfidf <- function(dt, col, sparsity = 0.995){
# make sure dt$col are clean and readable data
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(stripWhitespace) %>%
# remove stopwords before removing punctuationo so that stopwords like
# it's and i'll can be removed
tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument)
tf_mtx <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
tf_dt <- tf_mtx%>%
as.matrix() %>%
as.data.table()
tfidf_mtx <- DocumentTermMatrix(
corpus,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
) %>%
removeSparseTerms(sparsity)
tfidf_dt <- tfidf %>%
as.matrix() %>%
as.data.table()
return(list(tf_mtx = tf_mtx,
tf_dt = tf_dt,
tfidf_mtx = tfidf_mtx,
tfidf_dt = tfidf_dt))
}
aaa <- get_tfidf(note_bows, "medical_note")
# Follow this
# https://cran.r-project.org/web/packages/textmineR/vignettes/b_document_clustering.html
tfidf_mtx = aaa[[tfidf_mtx]]
# Follow this
# https://cran.r-project.org/web/packages/textmineR/vignettes/b_document_clustering.html
tfidf_mtx = aaa[["tfidf_mtx"]]
dissimilarity(tfidf_mtx, method = "cosine")
library(data.table)
library(magrittr)
library(tm)
library(caret)
library(progress)
load("./shiny-apps/RData/saved.RData")
note_bows[, target := ifelse(sample_type == "Gastroenterology", 1, 2)]
target <- note_bows$target
# Using the full text tf and tfidf =============================================
# clean up the document
dt <- note_bows[, medical_note := str_replace_all(medical_note, "\\.", " ")]
library(stringr)
# Using the full text tf and tfidf =============================================
# clean up the document
dt <- note_bows[, medical_note := str_replace_all(medical_note, "\\.", " ")]
get_tfidf <- function(dt, col, sparsity = 0.995){
# make sure dt$col are clean and readable data
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(stripWhitespace) %>%
# remove stopwords before removing punctuationo so that stopwords like
# it's and i'll can be removed
tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument)
tf_mtx <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
tf_dt <- tf_mtx%>%
as.matrix() %>%
as.data.table()
tfidf_mtx <- DocumentTermMatrix(
corpus,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
) %>%
removeSparseTerms(sparsity)
tfidf_dt <- tfidf %>%
as.matrix() %>%
as.data.table()
return(list(tf_mtx = tf_mtx,
tf_dt = tf_dt,
tfidf_mtx = tfidf_mtx,
tfidf_dt = tfidf_dt))
}
aaa <- get_tfidf(note_bows, "medical_note")
get_tfidf <- function(dt, col, sparsity = 0.995){
# make sure dt$col are clean and readable data
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(stripWhitespace) %>%
# remove stopwords before removing punctuationo so that stopwords like
# it's and i'll can be removed
tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument)
tf_mtx <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
tf_dt <- tf_mtx%>%
as.matrix() %>%
as.data.table()
tfidf_mtx <- DocumentTermMatrix(
corpus,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
) %>%
removeSparseTerms(sparsity)
tfidf_dt <- tfidf_mtx %>%
as.matrix() %>%
as.data.table()
return(list(tf_mtx = tf_mtx,
tf_dt = tf_dt,
tfidf_mtx = tfidf_mtx,
tfidf_dt = tfidf_dt))
}
aaa <- get_tfidf(note_bows, "medical_note")
tfidf_mtx <- aaa["tfidf_mtx"]
tfidf <- aaa[["tfidf"]] %>%
as.matrix()
View(tfidf_mtx)
col = "medical_note"
# make sure dt$col are clean and readable data
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(stripWhitespace) %>%
# remove stopwords before removing punctuationo so that stopwords like
# it's and i'll can be removed
tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument)
inspect(corpus[[1]])
tf_mtx <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
sparsity = 0.995
tf_mtx <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity)
bbb = as.matrix(tf_mtx)
View(bbb)
tfidf_mtx <- DocumentTermMatrix(
corpus,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
) %>%
removeSparseTerms(sparsity)
get_tfidf <- function(dt, col, sparsity = 0.995){
# make sure dt$col are clean and readable data
corpus = Corpus(VectorSource(dt[, get(col)])) %>%
tm_map(tolower) %>%
tm_map(stripWhitespace) %>%
# remove stopwords before removing punctuationo so that stopwords like
# it's and i'll can be removed
tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument)
tf_mtx <- DocumentTermMatrix(corpus) %>%
removeSparseTerms(sparsity) %>%
as.matrix()
tf_dt <- tf_mtx %>%
as.data.table()
tfidf_mtx <- DocumentTermMatrix(
corpus,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
) %>%
removeSparseTerms(sparsity) %>%
as.matrix()
tfidf_dt <- tfidf_mtx %>%
as.data.table()
return(list(tf_matrix = tf_mtx,
tf_datatable = tf_dt,
tfidf_matrix = tfidf_mtx,
tfidf_datatable = tfidf_dt))
}
aaa <- get_tfidf(note_bows, "medical_note")
tfidf_mtx <- aaa["tfidf_mtx"]
tfidf_mtx <- aaa[["tfidf_mtx"]]
tfidf_mtx <- aaa[["tfidf_matrix"]]
# Follow this
# https://cran.r-project.org/web/packages/textmineR/vignettes/b_document_clustering.html
tfidf_mtx = aaa[["tfidf_matrix"]]
# Follow this
# https://cran.r-project.org/web/packages/textmineR/vignettes/b_document_clustering.html
tfidf = aaa[["tfidf_matrix"]]
cos_sim <- tfidf %*% t(tfidf) / rowsum(tfidf * tfidf)
cos_sim <- tfidf %*% t(tfidf) / rowSums(tfidf * tfidf)
View(cos_sim)
?as.dist
dist <- as.dist(1 - cos_sim)
hc <- hclust(dist, "ward.D")
clustering <- cutree(hc, 2)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
plot(hc, main = "xxx",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 2, border = "red")
